{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503c7dff-e621-44bc-a335-8aa9f366c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Yonglong Tian (yonglong@mit.edu)\n",
    "Date: May 07, 2020\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "        #This part of the code uses GPU if present. Else, it uses CPU\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "        #this part of the code requires that the input feature dimension be 3 dimensional or larger.\n",
    "        #perhaps because the input data is in RGB format? \n",
    "        if len(features.shape) < 3: \n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19996fe-8895-4a32-aa2d-89880015eb17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3236, -0.5551],\n",
       "        [-2.0026,  0.0230]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = torch.randn(2,2)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "361c816b-2ec5-4c4d-a091-c596831e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:\n",
      " tensor([[0.4683, 1.1201],\n",
      "        [1.3249, 1.1536]])\n",
      "anchor_dot_contrast: \n",
      " tensor([[21.0554, 27.3226],\n",
      "        [27.3226, 44.0876]])\n"
     ]
    }
   ],
   "source": [
    "#batch_size\n",
    "bs = 4\n",
    "features = torch.randn(2, 2)\n",
    "contrast_feature  = features\n",
    "anchor_feature = contrast_feature\n",
    "#torch.randn: Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "\n",
    "temperature = 0.07\n",
    "anchor_feature = contrast_feature #Note we not doing exp their is a reason see below\n",
    "\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "\n",
    "#torch.div: Divides each element of the input 'input' by the corresponding element of 'other'.\n",
    "#torch.matmul: Matrix product of two tensors.\n",
    "\n",
    "print('feature:\\n {}'.format(features))\n",
    "print('anchor_dot_contrast: \\n {}'.format(anchor_dot_contrast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "633250e9-46a2-4916-bd9d-863c4a0595b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_dot_contrast: \n",
      "tensor([[35.6513, 13.5210],\n",
      "        [13.5210,  5.8937]])\n",
      "\n",
      "logits_max:\n",
      " tensor([[35.6513],\n",
      "        [13.5210]])\n",
      "\n",
      " logits:\n",
      " tensor([[  0.0000, -22.1303],\n",
      "        [  0.0000,  -7.6273]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('anchor_dot_contrast: \\n{}\\n'.format(anchor_dot_contrast))\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "print('logits_max:\\n {}\\n'.format(logits_max))\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "print(' logits:\\n {}\\n'.format(logits))#output see what happen to diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e9f91-aaaa-4f83-9d8b-4fdc0fbf7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "print('batch size:', bs)\n",
    "\n",
    "temperature = 0.07\n",
    "\n",
    "labels = torch.randint(4, (1,4))\n",
    "print('labels:', labels)\n",
    "\n",
    "mask = torch.eq(labels, labels.T).float()\n",
    "print('mask = \\n {}'.format(mask))#hard coding it for easier understanding otherwise its features.shape[1]\n",
    "# torch.eq: Computes element-wise equality\n",
    "# example: torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\n",
    "# output: tensor([[ True, False],\n",
    "#                [False, True]])\n",
    "\n",
    "\n",
    "contrast_count = 2\n",
    "\n",
    "anchor_count = contrast_count\n",
    "\n",
    "mask = mask.repeat(anchor_count, contrast_count)# mask-out self-contrast cases\n",
    "#tensor.repeat: Repeats this tensor along the specified dimensions.\n",
    "\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask), #Returns a tensor filled with the scalar value 1, with the same size as input.\n",
    "    1,\n",
    "    torch.arange(bs * anchor_count).view(-1, 1), #Returns a 1-D tensor of size [(end-stop)/step] with values from the \n",
    "                                                #interval [start, end) taken with common difference step beginning from start\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17790c1b-513e-475c-88d4-5e5f1f9be17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.ones_like(mask):\n",
      " tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "torch.arange(bs*anchor_count.view(-1.1)):\n",
      " tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6],\n",
      "        [7]])\n",
      "logits_mask\n",
      ": tensor([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask), #Returns a tensor filled with the scalar value 1, with the same size as input.\n",
    "    1,\n",
    "    torch.arange(bs * anchor_count).view(-1, 1), \n",
    "    #Returns a 1-D tensor of size [(end-stop)/step] with values from the \n",
    "    #interval [start, end) taken with common difference step beginning from start\n",
    "    0\n",
    ")\n",
    "print(\"torch.ones_like(mask):\\n {}\".format(torch.ones_like(mask)))\n",
    "print(\"torch.arange(bs*anchor_count.view(-1.1)):\\n {}\".format(torch.torch.arange(bs*anchor_count).view(-1,1)))\n",
    "print(\"logits_mask\\n: {}\".format(logits_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc79af38-12b0-4160-9310-d471ccd4abcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask * logits_mask = \n",
      "tensor([[0., 1., 0., 1., 1., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 1., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 0., 1., 0., 1.],\n",
      "        [1., 1., 0., 1., 1., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 1., 1., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = mask * logits_mask\n",
    "print('mask * logits_mask = \\n{}'.format(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97655e81-b269-4963-a42e-1de0bbf23e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(bs*anchor_count).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7da2e2c-4656-4293-a032-9c735fbf765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "labels_tensor = ([[3, 0, 2, 3]])\n",
    "#what above means in this particular batch of 4 we got 3,0,2,3 labels. \n",
    "#Just in case you forgot we are contrasting here only once so we will have 3_c, 0_c, 2_c, 3_c \n",
    "#as our contrast in the input batch.\n",
    "\n",
    "#basically 'batch_size' X 'contrast_count' X 'C' x 'Width' X 'height' -> check above if you confused mask = \n",
    "#tensor([[0., 1., 1., 1., 1., 1., 1., 1.],\n",
    "#        [1., 0., 1., 1., 1., 1., 1., 1.],\n",
    "#        [1., 1., 0., 1., 1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 0., 1., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1., 0., 1., 1., 1.],\n",
    "#        [1., 1., 1., 1., 1., 0., 1., 1.],\n",
    "#        [1., 1., 1., 1., 1., 1., 0., 1.],\n",
    "#        [1., 1., 1., 1., 1., 1., 1., 0.]])\n",
    "#Easy to understand the Self Supervised Contrastive Loss now which is simpler than this\n",
    "\n",
    "\n",
    "#this is really important so we created a mask = mask * logits_mask which tells us for \n",
    "#0 th image representation which are the image it should be contrasted with.\n",
    "# so our labels are labels tensor([[3, 0, 2, 3]])\n",
    "\n",
    "# I am renaming them for better understanding tensor([[3_1, 0_1, 2_1, 3_2]])\n",
    "# so at 3_0 will be contrasted with its own augmentation which is at position 5 (index = 4) and \n",
    "# position 8 (index = 7) in the first row those are the position marked one else its zero\n",
    "#See the image bellow for better understanding\n",
    "\n",
    "#mask * logits_mask = \n",
    "#tensor([[0., 0., 0., 1., 1., 0., 0., 1.],\n",
    "#        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "#        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "#        [1., 0., 0., 0., 1., 0., 0., 1.],\n",
    "#        [1., 0., 0., 1., 0., 0., 0., 1.],\n",
    "#        [0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "#        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "#        [1., 0., 0., 1., 1., 0., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf17db8-6318-4356-a5b8-22e7161517ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-01f7a4bcaf46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m anchor_dot_contrast = torch.div(\n\u001b[0;32m     11\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrast_feature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     self.temperature)\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# for numerical stability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mlogits_max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor_dot_contrast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(          \n",
    "    torch.matmul(anchor_feature, contrast_feature.T),        \n",
    "    self.temperature)\n",
    "#torch.div: Divides each element of the input 'input' by the corresponding element of 'other'.\n",
    "#torch.matmul: #Matrix product of two tensors.\n",
    "\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True) \n",
    "#Returns the maximum value for each row.\n",
    "#keepdim (bool): whether the output tensor has dim retained or not. Default is False\n",
    "\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "#Tensor.detach() is used to detach a tensor from the current computational graph. It returns a new tensor that doesn't require a gradient.\n",
    "\n",
    "\n",
    "# tile mask\n",
    "mask = mask.repeat(anchor_count, contrast_count)\n",
    "#Repeats this tensor along the specified dimensions\n",
    "\n",
    "# mask-out self-contrast cases\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "    0\n",
    ")\n",
    "mask = mask * logits_mask\n",
    "\n",
    "# compute log_prob\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "# compute mean of log-likelihood over positive\n",
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "# loss\n",
    "loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e88bad3-5f1e-4eb1-86b5-ec6ee562b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading torch-1.12.0-cp38-cp38-win_amd64.whl (161.9 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.0-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-0.12.0-cp38-cp38-win_amd64.whl (969 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\a\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: requests in c:\\users\\a\\anaconda3\\lib\\site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\a\\anaconda3\\lib\\site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\a\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\a\\anaconda3\\lib\\site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\a\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\a\\anaconda3\\lib\\site-packages (from requests->torchvision) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\a\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.12.0 torchaudio-0.12.0 torchvision-0.13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
